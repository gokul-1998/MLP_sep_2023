{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification image classification with Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,precision_recall_curve\n",
    "from sklearn.metrics import precision_score,recall_score,classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,cross_val_predict\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MinmaxScalar\n",
    "    - from sklearn.preprocessing import MinMaxScaler\n",
    "    - it scales the data between 0 and 1\n",
    "    - formula: (x - min)/(max - min)\n",
    "    - it is sensitive to outliers\n",
    "    - example usage:\n",
    "        - scaler = MinMaxScaler()\n",
    "        - scaler.fit(data)\n",
    "        - scaler.transform(data)\n",
    "- StandardScalar\n",
    "    - from sklearn.preprocessing import StandardScaler\n",
    "    - it scales the data with mean 0 and standard deviation 1\n",
    "    - formula: (x - mean)/std\n",
    "    - it is not sensitive to outliers\n",
    "    - example usage:\n",
    "        - scaler = StandardScaler()\n",
    "        - scaler.fit(data)\n",
    "        - scaler.transform(data)\n",
    "- Pipeline\n",
    "    - from sklearn.pipeline import Pipeline\n",
    "    - it is used to apply multiple transformations on the data\n",
    "    - example usage:\n",
    "        - pipeline = Pipeline([('scaler1', StandardScaler()), ('scaler2', MinMaxScaler())])\n",
    "        - pipeline.fit(data)\n",
    "        - pipeline.transform(data)\n",
    "- perceptron\n",
    "    - from sklearn.linear_model import Perceptron\n",
    "    - it is used to create a perceptron model\n",
    "    - example usage:\n",
    "        - perceptron = Perceptron()\n",
    "        - perceptron.fit(X_train, y_train)\n",
    "        - perceptron.predict(X_test)\n",
    "    - the perceptron model is a linear classifier, it is used to classify the data into two classes, works well with linearly separable data only\n",
    "- hingeloss\n",
    "    - from sklearn.metrics import hinge_loss\n",
    "    - it is used to calculate the hinge loss\n",
    "    - example usage:\n",
    "        - hinge_loss(y_true, y_pred)\n",
    "    - what is hinge loss?\n",
    "        - it is a loss function used for training classifiers\n",
    "        - it is used to maximize the margin between the data points and the decision boundary\n",
    "        - it is used in SVMs\n",
    "        - https://youtu.be/CL2pUVLB7eI?t=557\n",
    "        - mainly used for  maximum-margin classification, which also includes support vector machines\n",
    "        - ![](2023-08-16-08-59-53.png)\n",
    "- precision recall curve\n",
    "    - from sklearn.metrics import precision_recall_curve\n",
    "    - it is used to calculate the precision recall curve, it is used to find the best threshold for the model\n",
    "    - precision: tp/(tp + fp)\n",
    "    - recall: tp/(tp + fn)\n",
    "    - example usage:\n",
    "        - precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    - https://youtu.be/fF0MvCPsEp8?t=190\n",
    "- roc curve\n",
    "    - from sklearn.metrics import roc_curve\n",
    "    - it is used to calculate the roc curve, it is used to find the best threshold for the model\n",
    "    - roc should not be used for imbalanced datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- handwritten digit classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first we will do binary classification for class 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokulakrishnan/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X,y\u001b[39m=\u001b[39mfetch_openml(\u001b[39m'\u001b[39;49m\u001b[39mmnist_784\u001b[39;49m\u001b[39m'\u001b[39;49m,version\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,return_X_y\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:1118\u001b[0m, in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39m# obtain the data\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m url \u001b[39m=\u001b[39m _DATA_FILE\u001b[39m.\u001b[39mformat(data_description[\u001b[39m\"\u001b[39m\u001b[39mfile_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1118\u001b[0m bunch \u001b[39m=\u001b[39m _download_data_to_bunch(\n\u001b[1;32m   1119\u001b[0m     url,\n\u001b[1;32m   1120\u001b[0m     return_sparse,\n\u001b[1;32m   1121\u001b[0m     data_home,\n\u001b[1;32m   1122\u001b[0m     as_frame\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(as_frame),\n\u001b[1;32m   1123\u001b[0m     openml_columns_info\u001b[39m=\u001b[39;49mfeatures_list,\n\u001b[1;32m   1124\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1125\u001b[0m     target_columns\u001b[39m=\u001b[39;49mtarget_columns,\n\u001b[1;32m   1126\u001b[0m     data_columns\u001b[39m=\u001b[39;49mdata_columns,\n\u001b[1;32m   1127\u001b[0m     md5_checksum\u001b[39m=\u001b[39;49mdata_description[\u001b[39m\"\u001b[39;49m\u001b[39mmd5_checksum\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1128\u001b[0m     n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[1;32m   1129\u001b[0m     delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[1;32m   1130\u001b[0m     parser\u001b[39m=\u001b[39;49mparser_,\n\u001b[1;32m   1131\u001b[0m     read_csv_kwargs\u001b[39m=\u001b[39;49mread_csv_kwargs,\n\u001b[1;32m   1132\u001b[0m )\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m return_X_y:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m bunch\u001b[39m.\u001b[39mdata, bunch\u001b[39m.\u001b[39mtarget\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:669\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m ParserError\n\u001b[1;32m    667\u001b[0m     no_retry_exception \u001b[39m=\u001b[39m ParserError\n\u001b[0;32m--> 669\u001b[0m X, y, frame, categories \u001b[39m=\u001b[39m _retry_with_clean_cache(\n\u001b[1;32m    670\u001b[0m     url, data_home, no_retry_exception\n\u001b[1;32m    671\u001b[0m )(_load_arff_response)(\n\u001b[1;32m    672\u001b[0m     url,\n\u001b[1;32m    673\u001b[0m     data_home,\n\u001b[1;32m    674\u001b[0m     parser\u001b[39m=\u001b[39;49mparser,\n\u001b[1;32m    675\u001b[0m     output_type\u001b[39m=\u001b[39;49moutput_type,\n\u001b[1;32m    676\u001b[0m     openml_columns_info\u001b[39m=\u001b[39;49mfeatures_dict,\n\u001b[1;32m    677\u001b[0m     feature_names_to_select\u001b[39m=\u001b[39;49mdata_columns,\n\u001b[1;32m    678\u001b[0m     target_names_to_select\u001b[39m=\u001b[39;49mtarget_columns,\n\u001b[1;32m    679\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    680\u001b[0m     md5_checksum\u001b[39m=\u001b[39;49mmd5_checksum,\n\u001b[1;32m    681\u001b[0m     n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[1;32m    682\u001b[0m     delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[1;32m    683\u001b[0m     read_csv_kwargs\u001b[39m=\u001b[39;49mread_csv_kwargs,\n\u001b[1;32m    684\u001b[0m )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m Bunch(\n\u001b[1;32m    687\u001b[0m     data\u001b[39m=\u001b[39mX,\n\u001b[1;32m    688\u001b[0m     target\u001b[39m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m     target_names\u001b[39m=\u001b[39mtarget_columns,\n\u001b[1;32m    693\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:59\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m URLError:\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:534\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m arff_params: Dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    525\u001b[0m     parser\u001b[39m=\u001b[39mparser,\n\u001b[1;32m    526\u001b[0m     output_type\u001b[39m=\u001b[39moutput_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m     read_csv_kwargs\u001b[39m=\u001b[39mread_csv_kwargs \u001b[39mor\u001b[39;00m {},\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    533\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     X, y, frame, categories \u001b[39m=\u001b[39m _open_url_and_load_gzip_file(\n\u001b[1;32m    535\u001b[0m         url, data_home, n_retries, delay, arff_params\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m parser \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_openml.py:522\u001b[0m, in \u001b[0;36m_load_arff_response.<locals>._open_url_and_load_gzip_file\u001b[0;34m(url, data_home, n_retries, delay, arff_params)\u001b[0m\n\u001b[1;32m    520\u001b[0m gzip_file \u001b[39m=\u001b[39m _open_openml_url(url, data_home, n_retries\u001b[39m=\u001b[39mn_retries, delay\u001b[39m=\u001b[39mdelay)\n\u001b[1;32m    521\u001b[0m \u001b[39mwith\u001b[39;00m closing(gzip_file):\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mreturn\u001b[39;00m load_arff_from_gzip_file(gzip_file, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marff_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_arff_parser.py:520\u001b[0m, in \u001b[0;36mload_arff_from_gzip_file\u001b[0;34m(gzip_file, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a compressed ARFF file using a given parser.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m    `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m parser \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mliac-arff\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 520\u001b[0m     \u001b[39mreturn\u001b[39;00m _liac_arff_parser(\n\u001b[1;32m    521\u001b[0m         gzip_file,\n\u001b[1;32m    522\u001b[0m         output_type,\n\u001b[1;32m    523\u001b[0m         openml_columns_info,\n\u001b[1;32m    524\u001b[0m         feature_names_to_select,\n\u001b[1;32m    525\u001b[0m         target_names_to_select,\n\u001b[1;32m    526\u001b[0m         shape,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[39melif\u001b[39;00m parser \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_arff_parser(\n\u001b[1;32m    530\u001b[0m         gzip_file,\n\u001b[1;32m    531\u001b[0m         output_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         read_csv_kwargs,\n\u001b[1;32m    536\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/datasets/_arff_parser.py:199\u001b[0m, in \u001b[0;36m_liac_arff_parser\u001b[0;34m(gzip_file, output_arrays_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape)\u001b[0m\n\u001b[1;32m    196\u001b[0m dfs \u001b[39m=\u001b[39m [first_df[columns_to_keep]]\n\u001b[1;32m    197\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m _chunk_generator(arff_container[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m], chunksize):\n\u001b[1;32m    198\u001b[0m     dfs\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 199\u001b[0m         pd\u001b[39m.\u001b[39;49mDataFrame(data, columns\u001b[39m=\u001b[39;49mcolumns_names, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[columns_to_keep]\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[39m# dfs[0] contains only one row, which may not have enough data to infer to\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# column's dtype. Here we use `dfs[1]` to configure the dtype in dfs[0]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 746\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    747\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    748\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m         data,\n\u001b[1;32m    750\u001b[0m         columns,\n\u001b[1;32m    751\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m         dtype,\n\u001b[1;32m    753\u001b[0m     )\n\u001b[1;32m    754\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    755\u001b[0m         arrays,\n\u001b[1;32m    756\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m--> 510\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    511\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:975\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m--> 975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    977\u001b[0m \u001b[39mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1060\u001b[0m, in \u001b[0;36m_convert_object_array\u001b[0;34m(content, dtype)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         arr \u001b[39m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1058\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1060\u001b[0m arrays \u001b[39m=\u001b[39m [convert(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m content]\n\u001b[1;32m   1062\u001b[0m \u001b[39mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1060\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         arr \u001b[39m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1058\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1060\u001b[0m arrays \u001b[39m=\u001b[39m [convert(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m content]\n\u001b[1;32m   1062\u001b[0m \u001b[39mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1056\u001b[0m, in \u001b[0;36m_convert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(arr):\n\u001b[1;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1056\u001b[0m         arr \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_objects(arr)\n\u001b[1;32m   1057\u001b[0m         arr \u001b[39m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1058\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X,y=fetch_openml('mnist_784',version=1,return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.to_numpy()\n",
    "y=y.to_numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_names=np.unique(y)\n",
    "print('no of samples',len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
